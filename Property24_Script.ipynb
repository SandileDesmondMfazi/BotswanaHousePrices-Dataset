{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "Dataset =[]\n",
    "for page in range(1,12,1): #For loop to loop through all the property listing pages\n",
    "  html = urlopen(f'https://www.property24.co.bw/houses-for-sale-in-gaborone-c2250?Page={page}')\n",
    "  bs = BeautifulSoup(html.read(), 'html.parser')# Parsing the HTML\n",
    "  for Listing in bs.find_all('div',{'class':'propertyTileWrapper'}): #Find each property listing on the page\n",
    "    c+=1\n",
    "    title = Listing.find('div',{'class','area left'}).get_text(strip= True)#Element location for the Title \n",
    "    Erf_Size = Listing.find('span',{'class','sizeUnits'})#Element location for the Erf_Size\n",
    "    Item_Card = Listing.find('div',{'class','left detailsBorder detailsContainer'})#This element is used to identify the Bedroom, Bathroom, and Garages elements\n",
    "    Bedrooms = Item_Card.select('span')[0].get_text(strip= True)\n",
    "    #try blocks are used to manage the error output the the scraper doesn't find the elements.\n",
    "    try:\n",
    "      Bathrooms = Item_Card.select('span')[1].get_text(strip= True)\n",
    "    except:\n",
    "      Bathrooms = 'N/A'\n",
    "\n",
    "    try:\n",
    "      Garages = Item_Card.select('span')[2].get_text(strip= True)\n",
    "    except:\n",
    "      Garages = 'N/A'\n",
    "    \n",
    "    price = Listing.find('span',{'class':'price'}).get_text(strip= True)#element helps Identify the Price Location on the webpage\n",
    "    #created a dictionary to basically store the scraped data and later on convert it to a dataframe\n",
    "    Data = {\n",
    "        'Title': title,\n",
    "        'Erf_Size': Erf_Size,\n",
    "        'No_Of_Bedrooms': Bedrooms,\n",
    "        'No_Of_Bathrooms': Bathrooms,\n",
    "        'No_Of_Garages': Garages,\n",
    "        'Price': price \n",
    "    }\n",
    "    Dataset.append(Data)\n",
    "\n",
    "Dataset_Part1 = pd.DataFrame(Dataset)\n",
    "len(Dataset_Part1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_Part1.to_csv('Property24.csv', encoding='utf-8', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fca098a43d58bc592b99c7531737cc9fd1201fdc2aded6c1f466c075168ba7cb"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('minimal_ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
